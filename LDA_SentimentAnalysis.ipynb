{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce9c129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(r'Documents\\merged_tweets.csv', error_bad_lines=False);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "500aa0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>RT @1woo17: A gift from Jun for Valentine's Da...</td>\n",
       "      <td>PersephoneBasil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>RT @miwon17_: Event for Valentines day in Chin...</td>\n",
       "      <td>PersephoneBasil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>RT @mansehao: üê±: Hello, I am Confession Delive...</td>\n",
       "      <td>PersephoneBasil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>RT @tinkswonu: wonwoo and his own sound effect...</td>\n",
       "      <td>PersephoneBasil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>RT @SVTGlobal: More info:\\n\\nThank you, CARAT!...</td>\n",
       "      <td>PersephoneBasil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>RT @SVTGlobal: CARATs, please spread this info...</td>\n",
       "      <td>PersephoneBasil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>RT @svtachievements: US carats, here's an acco...</td>\n",
       "      <td>PersephoneBasil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>RT @cvtegyu: remember if you're from the US to...</td>\n",
       "      <td>PersephoneBasil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.0</td>\n",
       "      <td>RT @Gordito_Perrito: te amo infinito perrito y...</td>\n",
       "      <td>PersephoneBasil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>RT @minguhaeyo: US carats!!! Please consider b...</td>\n",
       "      <td>PersephoneBasil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              Tweet  \\\n",
       "0         0.0  RT @1woo17: A gift from Jun for Valentine's Da...   \n",
       "1         1.0  RT @miwon17_: Event for Valentines day in Chin...   \n",
       "2         2.0  RT @mansehao: üê±: Hello, I am Confession Delive...   \n",
       "3         3.0  RT @tinkswonu: wonwoo and his own sound effect...   \n",
       "4         4.0  RT @SVTGlobal: More info:\\n\\nThank you, CARAT!...   \n",
       "5         5.0  RT @SVTGlobal: CARATs, please spread this info...   \n",
       "6         6.0  RT @svtachievements: US carats, here's an acco...   \n",
       "7         7.0  RT @cvtegyu: remember if you're from the US to...   \n",
       "8         8.0  RT @Gordito_Perrito: te amo infinito perrito y...   \n",
       "9         9.0  RT @minguhaeyo: US carats!!! Please consider b...   \n",
       "\n",
       "          Username  \n",
       "0  PersephoneBasil  \n",
       "1  PersephoneBasil  \n",
       "2  PersephoneBasil  \n",
       "3  PersephoneBasil  \n",
       "4  PersephoneBasil  \n",
       "5  PersephoneBasil  \n",
       "6  PersephoneBasil  \n",
       "7  PersephoneBasil  \n",
       "8  PersephoneBasil  \n",
       "9  PersephoneBasil  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "164f2eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134925, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_value = float(\"NaN\")\n",
    "data.replace(\"\",nan_value,inplace = True)\n",
    "data = data.dropna()\n",
    "data.head()\n",
    "data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5efb2aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the tweets\n",
    "import numpy as np\n",
    "import re\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    try:\n",
    "        r = re.findall(pattern, input_txt)\n",
    "        #print(r)\n",
    "        for i in r:\n",
    "            input_txt = re.sub(i, '', input_txt)\n",
    "            #print(input_txt)\n",
    "        return input_txt\n",
    "    except:\n",
    "        print(\"An exception ocurred\")\n",
    "        \n",
    "def clean_tweets(tweets):\n",
    "    #remove twitter Return handles (RT @xxx:)\n",
    "    tweets = np.vectorize(remove_pattern)(tweets, \"RT @[\\w]*:\") \n",
    "    \n",
    "    #remove twitter handles (@xxx)\n",
    "    tweets = np.vectorize(remove_pattern)(tweets, \"@[\\w]*\")\n",
    "    \n",
    "    #remove URL links (httpxxx)\n",
    "    tweets = np.vectorize(remove_pattern)(tweets, \"https?://[A-Za-z0-9./]*\")\n",
    "    \n",
    "    #remove special characters, numbers, punctuations (except for #)\n",
    "    tweets = np.core.defchararray.replace(tweets, \"[^a-zA-Z]\", \" \")\n",
    "    \n",
    "    tweets = np.core.defchararray.replace(tweets, \"[A-Za-z0‚Äì9]+\", \" \")\n",
    "    \n",
    "    tweets = np.core.defchararray.replace(tweets, \"[‚Å∞-9A-Za-z \\t]\", \" \")\n",
    "    \n",
    "    tweets = np.core.defchararray.replace(tweets, \"[\\w+:\\/\\/\\S+]\", \" \")\n",
    "    \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e201cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Tweet'] = clean_tweets(data['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4de1eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-e8adb82cb448>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_text['index'] = data_text.index\n"
     ]
    }
   ],
   "source": [
    "data_text = data[['Tweet']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cf5b6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134925\n",
      "                                               Tweet  index\n",
      "0   A gift from Jun for Valentine's Day in China,...      0\n",
      "1   Event for Valentines day in China by Jun\\n520...      1\n",
      "2   üê±: Hello, I am Confession Delivery Company‚Äôs ...      2\n",
      "3   wonwoo and his own sound effects!! the cutest...      3\n",
      "4                More info:\\n\\nThank you, CARAT!\\n\\n      4\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d91fa863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Downloader.download of <nltk.downloader.Downloader object at 0x0000022012B1E430>>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9370bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AVuser\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17b78d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fed9240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to perform lemmatize and stem preprocessing steps on the data set.\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18a1511d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Tweet   index\n",
      "1700                                     Procrastination     1700\n",
      "1701     I was in Sec 114 last row and had great recep...    1701\n",
      "1702     Your Texans coverage is the best. Keep up the...    1702\n",
      "1703     With heavy hearts we announce the passing of ...    1703\n",
      "1704    David Eagleman: Can we create new senses for h...    1704\n",
      "...                                                   ...     ...\n",
      "137433  Tagging along with Credit Card Reform: More Fe...  137433\n",
      "137434  We may not be through with the flu - -dyn/cont...  137434\n",
      "137435  Tell Congress: Pass a good heathcare bill - in...  137435\n",
      "137436                reading from  ,8599,1966658,00.html  137436\n",
      "137437                                     reading from    137437\n",
      "\n",
      "[133225 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(documents[1700:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b53c6f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\AVuser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['David', 'Eagleman:', 'Can', 'we', 'create', 'new', 'senses', 'for', 'humans?', '#TED', ':', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['david', 'eagleman', 'creat', 'sens', 'human']\n"
     ]
    }
   ],
   "source": [
    "#Select a document to preview after preprocessing\n",
    "nltk.download('wordnet')\n",
    "stemmer = SnowballStemmer('english')\n",
    "doc_sample = documents[documents['index'] == 1704].values[0][0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17f9b76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [gift, valentin, china, fall, love, love, song]\n",
       "1    [event, valentin, china, confess, express, del...\n",
       "2    [hello, confess, deliveri, compani, deliveri, ...\n",
       "3        [wonwoo, sound, effect, cutest, bbmastopsoci]\n",
       "4                                 [info, thank, carat]\n",
       "5    [carat, spread, inform, choic, music, sale, re...\n",
       "6              [carat, account, definit, need, follow]\n",
       "7    [rememb, order, amazon, target, instead, count...\n",
       "8       [infinito, perrito, yoga, hecho, genial, sepa]\n",
       "9                         [carat, consid, buy, target]\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preprocess the tweets, saving the results as ‚Äòprocessed_docs‚Äô\n",
    "processed_docs = documents['Tweet'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70ed94a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 china\n",
      "1 fall\n",
      "2 gift\n",
      "3 love\n",
      "4 song\n",
      "5 valentin\n",
      "6 bbmastopsoci\n",
      "7 compani\n",
      "8 confess\n",
      "9 deliveri\n",
      "10 event\n"
     ]
    }
   ],
   "source": [
    "#Creating a dictionary from ‚Äòprocessed_docs‚Äô containing the number of times a word appears in the training set.\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99d7d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out tokens that appear in less than 15 documents (absolute number) or\n",
    "#more than 0.5 documents (fraction of total corpus size, not absolute number).\n",
    "#after the above two steps, keep only the first 100000 most frequent tokens.\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47a80896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(44, 1), (1195, 1), (1455, 1), (1722, 1)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For each document we create a dictionary reporting how many words and how many times those words appear\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[1704]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5012a176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 44 (\"creat\") appears 1 time.\n",
      "Word 1195 (\"david\") appears 1 time.\n",
      "Word 1455 (\"sens\") appears 1 time.\n",
      "Word 1722 (\"human\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "#Preview Bag Of Words for our sample preprocessed document.\n",
    "bow_doc_1704 = bow_corpus[1704]\n",
    "for i in range(len(bow_doc_1704)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_1704[i][0], \n",
    "                                               dictionary[bow_doc_1704[i][0]], \n",
    "bow_doc_1704[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1996ec54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.3465547617080078),\n",
      " (1, 0.35354381076943164),\n",
      " (2, 0.4096885613855012),\n",
      " (3, 0.4703592339313437),\n",
      " (4, 0.3947219935010614),\n",
      " (5, 0.4582778552102208)]\n"
     ]
    }
   ],
   "source": [
    "#Create tf-idf model object using models.TfidfModel on ‚Äòbow_corpus‚Äô and save it to ‚Äòtfidf‚Äô, then apply transformation to the entire corpus and call it ‚Äòcorpus_tfidf‚Äô. Finally we preview TF-IDF scores for our first document.\n",
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "005cd2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train our lda model using gensim.models.LdaMulticore and save it to ‚Äòlda_model‚Äô\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2638c325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.022*\"congratul\" + 0.019*\"coral\" + 0.017*\"love\" + 0.015*\"open\" + 0.015*\"team\" + 0.014*\"today\" + 0.014*\"amaz\" + 0.014*\"wonder\" + 0.013*\"great\" + 0.012*\"reef\"\n",
      "Topic: 1 \n",
      "Words: 0.027*\"covid\" + 0.017*\"chang\" + 0.016*\"work\" + 0.014*\"climat\" + 0.014*\"virus\" + 0.012*\"coronavirus\" + 0.012*\"health\" + 0.011*\"diseas\" + 0.010*\"model\" + 0.009*\"divers\"\n",
      "Topic: 2 \n",
      "Words: 0.026*\"para\" + 0.012*\"natur\" + 0.011*\"todo\" + 0.011*\"como\" + 0.010*\"sar\" + 0.010*\"pero\" + 0.010*\"india\" + 0.007*\"trial\" + 0.006*\"est√°\" + 0.006*\"beauti\"\n",
      "Topic: 3 \n",
      "Words: 0.024*\"trump\" + 0.014*\"twitter\" + 0.014*\"like\" + 0.013*\"presid\" + 0.010*\"record\" + 0.010*\"happen\" + 0.010*\"believ\" + 0.008*\"biden\" + 0.008*\"brilliant\" + 0.007*\"land\"\n",
      "Topic: 4 \n",
      "Words: 0.025*\"time\" + 0.021*\"best\" + 0.012*\"manag\" + 0.011*\"cool\" + 0.010*\"find\" + 0.009*\"research\" + 0.009*\"australian\" + 0.009*\"outcom\" + 0.009*\"real\" + 0.009*\"anim\"\n",
      "Topic: 5 \n",
      "Words: 0.013*\"studi\" + 0.009*\"genom\" + 0.009*\"year\" + 0.009*\"data\" + 0.009*\"covid\" + 0.008*\"work\" + 0.008*\"research\" + 0.007*\"like\" + 0.007*\"peopl\" + 0.007*\"time\"\n",
      "Topic: 6 \n",
      "Words: 0.056*\"thank\" + 0.039*\"great\" + 0.021*\"share\" + 0.018*\"talk\" + 0.017*\"work\" + 0.017*\"live\" + 0.015*\"research\" + 0.013*\"learn\" + 0.010*\"latest\" + 0.009*\"australia\"\n",
      "Topic: 7 \n",
      "Words: 0.031*\"look\" + 0.016*\"scienc\" + 0.015*\"women\" + 0.013*\"join\" + 0.012*\"research\" + 0.011*\"opportun\" + 0.010*\"posit\" + 0.010*\"forward\" + 0.009*\"avail\" + 0.009*\"scientist\"\n",
      "Topic: 8 \n",
      "Words: 0.022*\"read\" + 0.019*\"paper\" + 0.013*\"human\" + 0.012*\"week\" + 0.012*\"publish\" + 0.011*\"genet\" + 0.010*\"stori\" + 0.010*\"structur\" + 0.010*\"report\" + 0.010*\"book\"\n",
      "Topic: 9 \n",
      "Words: 0.022*\"good\" + 0.018*\"health\" + 0.016*\"need\" + 0.015*\"help\" + 0.013*\"peopl\" + 0.012*\"mental\" + 0.009*\"year\" + 0.009*\"young\" + 0.008*\"patient\" + 0.008*\"children\"\n"
     ]
    }
   ],
   "source": [
    "#For each topic, we will explore the words occuring in that topic and its relative weight.\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c35569a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ada32c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AVuser\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "prepare() missing 1 required positional argument: 'vectorizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-fffccf39f446>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: prepare() missing 1 required positional argument: 'vectorizer'"
     ]
    }
   ],
   "source": [
    "pyLDAvis.sklearn.prepare(lda_model, tfidf, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe0b977",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
